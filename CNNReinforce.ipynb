{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdffc895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d040060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(keras.Model):\n",
    "    \n",
    "    # Initialize network and archtitecture\n",
    "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=256):\n",
    "        super(PolicyGradientNetwork,self).__init__()\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims \n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.conv1 = Conv2D(filters=8, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool1 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.conv2 = Conv2D(filters=16, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool2 =MaxPool2D(pool_size=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.fc1 = Dense(self.fc1_dims, activation = 'relu')\n",
    "        # sself.fc2 = Dense(self.fc2_dims, activation = 'relu')\n",
    "        self.pi = Dense(self.n_actions, activation = 'softmax')\n",
    "    \n",
    "    # Forward Pass\n",
    "    def call(self,state):\n",
    "        \n",
    "        value = self.conv1(state)\n",
    "        value = self.maxpool1(value)\n",
    "        \n",
    "        value = self.conv2(value)\n",
    "        value = self.maxpool2(value)\n",
    "        \n",
    "        value = self.flatten(value)\n",
    "        \n",
    "        value = self.fc1(value)\n",
    "        #value = self.fc2(value)\n",
    "        pi = self.pi(value)\n",
    "        \n",
    "        return pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c6cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, alpha = 0.003, gamma = 0.99, n_actions = 4, fc1_dims = 256, fc2_dims = 256):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Keep tracks of the actions we take, the states we visit and the rewards we get \n",
    "        self.state_memory = [] \n",
    "        self.action_memory = [] \n",
    "        self.reward_memory = [] \n",
    "        \n",
    "        # Policy Network \n",
    "        self.policy = PolicyGradientNetwork(n_actions=n_actions)\n",
    "        self.policy.compile(optimizer=Adam(learning_rate=self.alpha))\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        # Convert numpy array to tensorflow tensor\n",
    "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "        \n",
    "        # Then we want to pass that state through the policy network \n",
    "        # Which gives us the probability distribution for choosing each action \n",
    "        probs = self.policy(state)\n",
    "        \n",
    "        # Used when you have a set of discrete classes and you want to choose one of classes with a particular proba\n",
    "        action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "        action = action_probs.sample()\n",
    "        \n",
    "        return action.numpy()[0]\n",
    "    \n",
    "    def store_transition(self, observation, action, reward):\n",
    "        # Stores the transitions of the current episode \n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    def learn(self):\n",
    "        # Now we need a function to actually perform the agents learning \n",
    "        \n",
    "        # Convert actions and rewards to tensors\n",
    "        actions = tf.convert_to_tensor(self.action_memory, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(self.reward_memory, dtype=tf.float32)\n",
    "        \n",
    "        # Calculate sum of future rewards which follow each time step \n",
    "        G = np.zeros_like(rewards)\n",
    "        # Now we iterate over the agents memory \n",
    "        for t in range(len(rewards)):\n",
    "            G_sum = 0 \n",
    "            discount = 1 # Present step \n",
    "            for k in range(t, len(rewards)): # Goes from current time step to end of the episode \n",
    "                G_sum += rewards[k]*discount\n",
    "                discount *= self.gamma\n",
    "            G[t]= G_sum \n",
    "        \n",
    "        # Now we can calculate the gradient at each time step in the episode \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 0 \n",
    "            # Now workign back through the agent's memory \n",
    "            for idx, (g,state) in enumerate(zip(G,self.state_memory)):\n",
    "                state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "                probs = self.policy(state)\n",
    "                action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "                # Log probability for the gradient calculation\n",
    "                log_prob = action_probs.log_prob(actions[idx])\n",
    "                loss += -g * tf.squeeze(log_prob)\n",
    "                \n",
    "        gradient = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        self.policy.optimizer.apply_gradients(zip(gradient, self.policy.trainable_variables))\n",
    "        \n",
    "        # Now at the end of every episode we need to zero out (empty) our agent's memory \n",
    "        \n",
    "        self.state_memory = [] \n",
    "        self.action_memory = [] \n",
    "        self.reward_memory = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a35424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import minihack\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nle import nethack\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411704e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-Quest-Hard-v0\", observation_keys=[\"glyphs\",\"pixel\"])\n",
    "#display_screen(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "905d627e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a166899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def glyph_array(state):\n",
    "    #return state['glyphs_crop'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe1c542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1659"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21*79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1458437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glyph_array(state):\n",
    "    #glyphs = state['glyphs'].flatten()\n",
    "    #glyphs = glyphs[glyphs!=2359]\n",
    "    glyphs= state['glyphs']\n",
    "    glyphs = glyphs/2383\n",
    "    glyphs = glyphs.reshape((21,79,1))\n",
    "    return glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df122d71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-27 19:41:21.067664: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-10-27 19:41:21.067786: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(alpha=0.0005, gamma =0.9, n_actions = env.action_space.n)\n",
    "\n",
    "score_history = [] \n",
    "n_episodes = 5000\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    done = False\n",
    "    score = 0 \n",
    "    observation = glyph_array(env.reset())\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        #print(action)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        observation_ = glyph_array(observation_)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_ \n",
    "        score += reward \n",
    "    score_history.append(score)\n",
    "    \n",
    "    agent.learn()\n",
    "    \n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n",
    "    \n",
    "plt.plot(score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20416941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def glyph_array(state):\n",
    "   #glyphs = state['glyphs'].flatten()\n",
    "    #glyphs = glyphs[glyphs!=2359]\n",
    "    #glyphs = glyphs/2383\n",
    "   # return glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_screen(state): # state in rgb \n",
    "    screen = Image.fromarray(np.uint8(state['pixel']))\n",
    "    display(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "done = False \n",
    "while not done:\n",
    "    display_screen(observation)\n",
    "    action = agent.choose_action(glyph_array(observation))\n",
    "    observation_, reward, done, info = env.step(action)\n",
    "    observation = observation_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6985d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110465bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:minihack]",
   "language": "python",
   "name": "conda-env-minihack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
