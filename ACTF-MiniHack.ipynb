{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41ec6225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import gym\n",
    "import minihack\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nle import nethack\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ebf251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, fc1_dims=1024, fc2_dims=512, name = 'actor_crtic', chkpt_dir = 'tmp/actor_critic'):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.model_name = name \n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ac')\n",
    "        \n",
    "        self.fc1 = Dense(self.fc1_dims, activation ='relu')\n",
    "        self.fc2 = Dense(self.fc2_dims, activation ='relu')\n",
    "        self.v = Dense(1, activation = None)\n",
    "        self.pi = Dense(n_actions, activation='softmax')\n",
    "        \n",
    "    def call(self, state):\n",
    "        value = self.fc1(state)\n",
    "        value = self.fc2(value)\n",
    "        \n",
    "        v = self.v(value)\n",
    "        pi = self.pi(value)\n",
    "        \n",
    "        return v,pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95027084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, alpha=0.003, gamma =0.99, n_actions = 2):\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.action = None \n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.actor_critic = ActorCriticNetwork(n_actions=n_actions)\n",
    "        self.actor_critic.compile(optimizer=Adam(learning_rate=alpha))\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        _, probs = self.actor_critic(state)\n",
    "        \n",
    "        action_probabilities = tfp.distributions.Categorical(probs=probs)\n",
    "        action = action_probabilities.sample()\n",
    "        self.action = action\n",
    "        \n",
    "        return action.numpy()[0]\n",
    "    \n",
    "    def save_models(self):\n",
    "        print('...saving models...')\n",
    "        self.actor_critic.save_weights(self.actor_critic.checkpoint_file)\n",
    "        \n",
    "    def load_models(self):\n",
    "        print('...loading models...')\n",
    "        self.actor_critic.load_weights(self.actor_critic.checkpoint_file)\n",
    "        \n",
    "    def learn(self, state, reward, state_, done):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        state_ = tf.convert_to_tensor([state_], dtype = tf.float32)\n",
    "        reward = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            state_value, probs = self.actor_critic(state)\n",
    "            state_value_, _ = self.actor_critic(state_)\n",
    "            state_value = tf.squeeze(state_value)\n",
    "            state_value_ = tf.squeeze(state_value_)\n",
    "            \n",
    "            action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "            log_prob = action_probs.log_prob(self.action)\n",
    "            \n",
    "            delta = reward + self.gamma*state_value_*(1-int(done)) - state_value \n",
    "            actor_loss = -log_prob*delta \n",
    "            critic_loss = delta**2\n",
    "            \n",
    "            total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        gradient = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
    "        self.actor_critic.optimizer.apply_gradients(zip(gradient, self.actor_critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36c9c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-Room-5x5-v0\", observation_keys=[\"glyphs\",\"pixel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26ab45fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glyph_array(state):\n",
    "    glyphs = state['glyphs'].flatten()\n",
    "    #glyphs = glyphs[glyphs!=2359]\n",
    "    glyphs = glyphs/2383\n",
    "    return glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a0d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-27 19:38:39.404396: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-10-27 19:38:39.404486: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...saving models...\n",
      "episode  0 score 0.9 avg_score 0.9\n",
      "episode  1 score -0.3 avg_score 0.3\n",
      "episode  2 score -0.4 avg_score 0.1\n",
      "episode  3 score 0.9 avg_score 0.3\n",
      "episode  4 score -0.4 avg_score 0.2\n",
      "episode  5 score -0.6 avg_score 0.0\n",
      "episode  6 score -0.7 avg_score -0.1\n",
      "episode  7 score -0.6 avg_score -0.2\n",
      "episode  8 score -0.6 avg_score -0.2\n",
      "episode  9 score 1.0 avg_score -0.1\n",
      "episode  10 score -0.3 avg_score -0.1\n",
      "episode  11 score -0.8 avg_score -0.2\n",
      "episode  12 score -0.8 avg_score -0.2\n",
      "episode  13 score -0.7 avg_score -0.2\n",
      "episode  14 score -0.7 avg_score -0.3\n",
      "episode  15 score -0.5 avg_score -0.3\n",
      "episode  16 score -0.5 avg_score -0.3\n",
      "episode  17 score -0.7 avg_score -0.3\n",
      "episode  18 score -0.7 avg_score -0.3\n",
      "episode  19 score -0.7 avg_score -0.4\n",
      "episode  20 score -0.4 avg_score -0.4\n",
      "episode  21 score 0.7 avg_score -0.3\n",
      "episode  22 score -0.3 avg_score -0.3\n",
      "episode  23 score 0.7 avg_score -0.3\n",
      "episode  24 score 1.0 avg_score -0.2\n",
      "episode  25 score -0.3 avg_score -0.2\n",
      "episode  26 score -0.8 avg_score -0.2\n",
      "episode  27 score -0.8 avg_score -0.3\n",
      "episode  28 score -0.8 avg_score -0.3\n",
      "episode  29 score -0.8 avg_score -0.3\n",
      "episode  30 score -0.7 avg_score -0.3\n",
      "episode  31 score -0.8 avg_score -0.3\n",
      "episode  32 score -0.7 avg_score -0.3\n",
      "episode  33 score -0.6 avg_score -0.3\n",
      "episode  34 score -0.4 avg_score -0.3\n",
      "episode  35 score -0.4 avg_score -0.3\n",
      "episode  36 score -0.4 avg_score -0.3\n",
      "episode  37 score -0.5 avg_score -0.4\n",
      "episode  38 score -0.8 avg_score -0.4\n",
      "episode  39 score -0.9 avg_score -0.4\n",
      "episode  40 score -0.9 avg_score -0.4\n",
      "episode  41 score -0.9 avg_score -0.4\n",
      "episode  42 score -0.9 avg_score -0.4\n",
      "episode  43 score -0.9 avg_score -0.4\n",
      "episode  44 score -0.9 avg_score -0.4\n",
      "episode  45 score -0.8 avg_score -0.4\n",
      "episode  46 score -0.8 avg_score -0.5\n",
      "episode  47 score -0.8 avg_score -0.5\n",
      "episode  48 score -0.8 avg_score -0.5\n",
      "episode  49 score -1.0 avg_score -0.5\n",
      "episode  50 score -0.9 avg_score -0.5\n",
      "episode  51 score -1.0 avg_score -0.5\n",
      "episode  52 score -0.9 avg_score -0.5\n",
      "episode  53 score -1.0 avg_score -0.5\n",
      "episode  54 score -1.0 avg_score -0.5\n",
      "episode  55 score -0.9 avg_score -0.5\n",
      "episode  56 score -1.0 avg_score -0.5\n",
      "episode  57 score -1.0 avg_score -0.5\n",
      "episode  58 score -1.0 avg_score -0.6\n",
      "episode  59 score -1.0 avg_score -0.6\n",
      "episode  60 score -1.0 avg_score -0.6\n",
      "episode  61 score -1.0 avg_score -0.6\n",
      "episode  62 score -1.0 avg_score -0.6\n",
      "episode  63 score -1.0 avg_score -0.6\n",
      "episode  64 score -1.0 avg_score -0.6\n",
      "episode  65 score -1.0 avg_score -0.6\n",
      "episode  66 score -1.0 avg_score -0.6\n",
      "episode  67 score -1.0 avg_score -0.6\n",
      "episode  68 score -1.0 avg_score -0.6\n",
      "episode  69 score -1.0 avg_score -0.6\n",
      "episode  70 score -0.9 avg_score -0.6\n",
      "episode  71 score -1.0 avg_score -0.6\n",
      "episode  72 score -1.0 avg_score -0.6\n",
      "episode  73 score -1.0 avg_score -0.6\n",
      "episode  74 score -1.0 avg_score -0.6\n",
      "episode  75 score -1.0 avg_score -0.6\n",
      "episode  76 score -1.0 avg_score -0.7\n",
      "episode  77 score -1.0 avg_score -0.7\n",
      "episode  78 score -1.0 avg_score -0.7\n",
      "episode  79 score -1.0 avg_score -0.7\n",
      "episode  80 score -1.0 avg_score -0.7\n",
      "episode  81 score -1.0 avg_score -0.7\n",
      "episode  82 score -1.0 avg_score -0.7\n",
      "episode  83 score -1.0 avg_score -0.7\n",
      "episode  84 score -1.0 avg_score -0.7\n",
      "episode  85 score -1.0 avg_score -0.7\n",
      "episode  86 score -1.0 avg_score -0.7\n",
      "episode  87 score -1.0 avg_score -0.7\n",
      "episode  88 score -1.0 avg_score -0.7\n",
      "episode  89 score -1.0 avg_score -0.7\n",
      "episode  90 score -1.0 avg_score -0.7\n",
      "episode  91 score -1.0 avg_score -0.7\n",
      "episode  92 score -1.0 avg_score -0.7\n",
      "episode  93 score -1.0 avg_score -0.7\n",
      "episode  94 score -1.0 avg_score -0.7\n",
      "episode  95 score -1.0 avg_score -0.7\n",
      "episode  96 score -1.0 avg_score -0.7\n",
      "episode  97 score -1.0 avg_score -0.7\n",
      "episode  98 score -1.0 avg_score -0.7\n",
      "episode  99 score -1.0 avg_score -0.7\n",
      "episode  100 score -1.0 avg_score -0.7\n",
      "episode  101 score -1.0 avg_score -0.8\n",
      "episode  102 score -1.0 avg_score -0.8\n",
      "episode  103 score -1.0 avg_score -0.8\n",
      "episode  104 score -1.0 avg_score -0.8\n",
      "episode  105 score -1.0 avg_score -0.8\n",
      "episode  106 score -1.0 avg_score -0.8\n",
      "episode  107 score -1.0 avg_score -0.8\n",
      "episode  108 score -1.0 avg_score -0.8\n",
      "episode  109 score -1.0 avg_score -0.8\n",
      "episode  110 score -1.0 avg_score -0.8\n",
      "episode  111 score -1.0 avg_score -0.8\n",
      "episode  112 score -1.0 avg_score -0.8\n",
      "episode  113 score -1.0 avg_score -0.8\n",
      "episode  114 score -1.0 avg_score -0.8\n",
      "episode  115 score -1.0 avg_score -0.8\n",
      "episode  116 score -1.0 avg_score -0.8\n",
      "episode  117 score -1.0 avg_score -0.9\n",
      "episode  118 score -1.0 avg_score -0.9\n",
      "episode  119 score -1.0 avg_score -0.9\n",
      "episode  120 score -1.0 avg_score -0.9\n",
      "episode  121 score -1.0 avg_score -0.9\n",
      "episode  122 score -1.0 avg_score -0.9\n",
      "episode  123 score -1.0 avg_score -0.9\n",
      "episode  124 score -1.0 avg_score -0.9\n",
      "episode  125 score -1.0 avg_score -0.9\n",
      "episode  126 score -0.9 avg_score -0.9\n",
      "episode  127 score -1.0 avg_score -0.9\n",
      "episode  128 score -1.0 avg_score -0.9\n",
      "episode  129 score -1.0 avg_score -0.9\n",
      "episode  130 score -1.0 avg_score -0.9\n",
      "episode  131 score -1.0 avg_score -0.9\n",
      "episode  132 score -1.0 avg_score -0.9\n",
      "episode  133 score -1.0 avg_score -1.0\n",
      "episode  134 score -1.0 avg_score -1.0\n",
      "episode  135 score -1.0 avg_score -1.0\n",
      "episode  136 score -1.0 avg_score -1.0\n",
      "episode  137 score -1.0 avg_score -1.0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(alpha = 1e-5, n_actions = env.action_space.n)\n",
    "n_games= 2000 \n",
    "\n",
    "filename = 'cartpole.png'\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = [] \n",
    "load_checkpoint = False \n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    \n",
    "for i in range(n_games):\n",
    "    observation = glyph_array(env.reset())\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        #print(observation)\n",
    "        action = agent.choose_action(observation)\n",
    "        #print(action)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        observation_ = glyph_array(observation_)\n",
    "        score += reward \n",
    "        if not load_checkpoint:\n",
    "            agent.learn(observation, reward, observation_, done)\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "            \n",
    "    print('episode ', i, 'score %.1f' % score, 'avg_score %.1f' % avg_score )\n",
    "\n",
    "x = [i+1 for i in range(n_games)]\n",
    "plt.plot(x,score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be334636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e62ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f599e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:minihack]",
   "language": "python",
   "name": "conda-env-minihack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
