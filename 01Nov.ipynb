{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdffc895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from nle import nethack\n",
    "import gym\n",
    "import minihack\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nle import nethack\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d040060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(keras.Model):\n",
    "    \n",
    "    # Initialize network and archtitecture\n",
    "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=128, fc3_dims = 256, fc4_dims=128, fc5_dims=64):\n",
    "        super(PolicyGradientNetwork,self).__init__()\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims \n",
    "        self.fc3_dims = fc3_dims\n",
    "        self.fc4_dims = fc4_dims \n",
    "        self.fc5_dims = fc5_dims \n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        \n",
    "        # Conv for full glyphs \n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool1 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.conv2 = Conv2D(filters=16, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool2 =MaxPool2D(pool_size=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        \n",
    "        # Conv for cropped glyphs \n",
    "        self.conv3= Conv2D(filters=32, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool3 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.conv4 = Conv2D(filters=16, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool4 =MaxPool2D(pool_size=(2, 2))\n",
    "        self.flatten_crop= Flatten()\n",
    "        \n",
    "\n",
    "        self.fc1 = Dense(self.fc1_dims, activation = 'relu')\n",
    "        self.fc2 = Dense(self.fc2_dims, activation = 'relu')\n",
    "        self.fc3 = Dense(self.fc3_dims, activation = 'relu')\n",
    "        self.fc4 = Dense(self.fc4_dims, activation = 'relu')\n",
    "        self.fc5 = Dense(self.fc5_dims, activation = 'relu')\n",
    "        \n",
    "        \n",
    "        self.pi = Dense(self.n_actions, activation = 'softmax')\n",
    "    \n",
    "    # Forward Pass\n",
    "    def call(self,state):\n",
    "        \n",
    "        glyphs_t = tf.convert_to_tensor([state[\"glyphs\"]], dtype=tf.float32)\n",
    "        glyphs_cropped_t = tf.convert_to_tensor([state[\"glyphs_crop\"]], dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        stats_t = tf.convert_to_tensor([state[\"stats\"]], dtype=tf.float32)\n",
    "        inv_glyphs_t = tf.convert_to_tensor([state[\"inv_glyphs\"]], dtype=tf.float32)\n",
    "        stats_inv_t = tf.concat([stats_t,inv_glyphs_t], 1)\n",
    "        \n",
    "        message_t = tf.convert_to_tensor([state[\"message\"]], dtype=tf.float32)\n",
    "        stats_inv_mess_t = tf.concat([stats_inv_t,message_t], 1)\n",
    "        \n",
    "        # Full\n",
    "        conv_value = self.conv1(glyphs_t)\n",
    "        conv_value = self.maxpool1(conv_value)\n",
    "        conv_value = self.conv2(conv_value)\n",
    "        conv_value = self.maxpool2(conv_value)\n",
    "        conv_value = self.flatten(conv_value)\n",
    "        \n",
    "        # Cropped \n",
    "        conv_value_crop = self.conv3(glyphs_cropped_t)\n",
    "        conv_value_crop = self.maxpool3(conv_value_crop)\n",
    "        conv_value_crop = self.conv4(conv_value_crop)\n",
    "        conv_value_crop = self.maxpool4(conv_value_crop)\n",
    "        conv_value_crop = self.flatten_crop(conv_value_crop)\n",
    "        \n",
    "        # Inclusion of Stats, Inv_glpyhs, Message \n",
    "        fc_value = self.fc1(stats_inv_mess_t)\n",
    "        fc_value = self.fc2(fc_value)\n",
    "        \n",
    "        value = tf.concat([conv_value,conv_value_crop, fc_value],1)\n",
    "\n",
    "        value = self.fc3(value)\n",
    "        value = self.fc4(value)\n",
    "        value = self.fc5(value)\n",
    "        \n",
    "        pi = self.pi(value)\n",
    "        \n",
    "        return pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c6cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, alpha = 0.003, gamma = 0.99, n_actions = 4):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Keep tracks of the actions we take, the states we visit and the rewards we get \n",
    "        self.state_memory = [] \n",
    "        self.action_memory = [] \n",
    "        self.reward_memory = [] \n",
    "        \n",
    "        # Policy Network \n",
    "        self.policy = PolicyGradientNetwork(n_actions=n_actions)\n",
    "        self.policy.compile(optimizer=Adam(learning_rate=self.alpha))\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        # Convert numpy array to tensorflow tensor\n",
    "        #state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "        state = observation\n",
    "        \n",
    "        # Then we want to pass that state through the policy network \n",
    "        # Which gives us the probability distribution for choosing each action \n",
    "        probs = self.policy(state)\n",
    "        \n",
    "        # Used when you have a set of discrete classes and you want to choose one of classes with a particular proba\n",
    "        action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "        action = action_probs.sample()\n",
    "        \n",
    "        return action.numpy()[0]\n",
    "    \n",
    "    def store_transition(self, observation, action, reward):\n",
    "        # Stores the transitions of the current episode \n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    def learn(self):\n",
    "        # Now we need a function to actually perform the agents learning \n",
    "        \n",
    "        # Convert actions and rewards to tensors\n",
    "        actions = tf.convert_to_tensor(self.action_memory, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(self.reward_memory, dtype=tf.float32)\n",
    "        \n",
    "        # Calculate sum of future rewards which follow each time step \n",
    "        G = np.zeros_like(rewards)\n",
    "        # Now we iterate over the agents memory \n",
    "        for t in range(len(rewards)):\n",
    "            G_sum = 0 \n",
    "            discount = 1 # Present step \n",
    "            for k in range(t, len(rewards)): # Goes from current time step to end of the episode \n",
    "                G_sum += rewards[k]*discount\n",
    "                discount *= self.gamma\n",
    "            G[t]= G_sum \n",
    "        \n",
    "        # Now we can calculate the gradient at each time step in the episode \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 0 \n",
    "            # Now workign back through the agent's memory \n",
    "            for idx, (g,state) in enumerate(zip(G,self.state_memory)):\n",
    "               \n",
    "               \n",
    "                #state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "                probs = self.policy(state)\n",
    "                action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "                # Log probability for the gradient calculation\n",
    "                log_prob = action_probs.log_prob(actions[idx])\n",
    "                loss += -g * tf.squeeze(log_prob)\n",
    "                \n",
    "        gradient = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        #self.policy.optimizer.apply_gradients(zip(gradient, self.policy.trainable_variables))\n",
    "        self.policy.optimizer.apply_gradients((grad, var) for (grad, var) in zip(gradient, self.policy.trainable_variables) if grad is not None)\n",
    "        \n",
    "        # Now at the end of every episode we need to zero out (empty) our agent's memory \n",
    "        \n",
    "        self.state_memory = [] \n",
    "        self.action_memory = [] \n",
    "        self.reward_memory = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d2a2257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glyph_array(state):\n",
    "    #glyphs = state['glyphs'].flatten()\n",
    "    #glyphs = glyphs[glyphs!=2359]\n",
    "    glyphs= state['glyphs']\n",
    "    glyphs = glyphs/2383\n",
    "    glyphs = glyphs.reshape((21,79,1))\n",
    "    return glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d48ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_screen(state): # state in rgb \n",
    "    screen = Image.fromarray(np.uint8(state['pixel']))\n",
    "    display(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e1b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#observation = [glyphs-2D, blstats1D, inv_glyphs-1D] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9520c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_obs(observation):\n",
    "    glyphs = np.array(observation[\"glyphs\"]/observation[\"glyphs\"].max())\n",
    "    glyphs = glyphs.reshape((21,79,1))\n",
    "    \n",
    "    glyphs_crop = np.array(observation[\"glyphs_crop\"]/observation[\"glyphs_crop\"].max())\n",
    "    glyphs_crop = glyphs_crop.reshape((9,9,1))\n",
    "    \n",
    "    stats = np.array(observation[\"blstats\"]/observation[\"blstats\"].max())\n",
    "    inv_glyphs = np.array(observation[\"inv_glyphs\"]/observation[\"inv_glyphs\"].max())\n",
    "    \n",
    "    message = np.array(observation[\"message\"]/observation[\"message\"].max())\n",
    "    \n",
    "    observation = {'glyphs':glyphs,'glyphs_crop':glyphs_crop, 'stats':stats,'inv_glyphs':inv_glyphs, 'message': message} \n",
    "    return observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4754a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVE_ACTIONS = tuple(nethack.CompassDirection)\n",
    "NAVIGATE_ACTIONS = MOVE_ACTIONS + (\n",
    "    nethack.Command.PICKUP,\n",
    "    nethack.Command.APPLY,\n",
    "    nethack.Command.FIRE,\n",
    "    nethack.Command.RUSH,\n",
    "    nethack.Command.ZAP, \n",
    "    nethack.Command.PUTON,\n",
    "    nethack.Command.READ, \n",
    "    nethack.Command.WEAR, \n",
    "    nethack.Command.QUAFF\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eefc45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MiniHack-LavaCross-Levitate-Potion-Inv-v0\", observation_keys=[\"glyphs\",\"glyphs_crop\",\"pixel\",\"blstats\",\"inv_glyphs\",\"message\"],\n",
    "              actions  = NAVIGATE_ACTIONS)\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c0637f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 CompassDirection.N\n",
      "1 CompassDirection.E\n",
      "2 CompassDirection.S\n",
      "3 CompassDirection.W\n",
      "4 CompassDirection.NE\n",
      "5 CompassDirection.SE\n",
      "6 CompassDirection.SW\n",
      "7 CompassDirection.NW\n",
      "8 Command.PICKUP\n",
      "9 Command.APPLY\n",
      "10 Command.FIRE\n",
      "11 Command.RUSH\n",
      "12 Command.ZAP\n",
      "13 Command.PUTON\n",
      "14 Command.READ\n",
      "15 Command.WEAR\n",
      "16 Command.QUAFF\n"
     ]
    }
   ],
   "source": [
    "for i in range(env.action_space.n):\n",
    "    print(i,env._actions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "411704e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1264x336 at 0x295234BB0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#env = gym.make(\"MiniHack-Room-5x5-v0\", observation_keys=[\"glyphs\",\"glyphs_crop\",\"pixel\",\"blstats\",\"inv_glyphs\",\"message\"])\n",
    "obs = env.reset()\n",
    "display_screen(obs)\n",
    "#obs[\"glyphs_crop\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df122d71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-01 20:06:25.739866: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-11-01 20:06:25.740934: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_70505/1743424471.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  message = np.array(observation[\"message\"]/observation[\"message\"].max())\n",
      "/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_70505/1743424471.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  stats = np.array(observation[\"blstats\"]/observation[\"blstats\"].max())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0 score -1.0 avg score -1.0\n",
      "episode  1 score -0.8 avg score -0.9\n",
      "episode  2 score -0.2 avg score -0.7\n",
      "episode  3 score -0.6 avg score -0.6\n",
      "episode  4 score -0.6 avg score -0.6\n",
      "episode  5 score -0.7 avg score -0.6\n",
      "episode  6 score 0.1 avg score -0.5\n",
      "episode  7 score -1.5 avg score -0.7\n",
      "episode  8 score -0.2 avg score -0.6\n",
      "episode  9 score 0.3 avg score -0.5\n",
      "episode  10 score 0.5 avg score -0.4\n",
      "episode  11 score -3.3 avg score -0.6\n",
      "episode  12 score 0.0 avg score -0.6\n",
      "episode  13 score -0.3 avg score -0.6\n",
      "episode  14 score -0.1 avg score -0.5\n",
      "episode  15 score -0.2 avg score -0.5\n",
      "episode  16 score -0.2 avg score -0.5\n",
      "episode  17 score -0.0 avg score -0.5\n",
      "episode  18 score -0.1 avg score -0.5\n",
      "episode  19 score -0.7 avg score -0.5\n",
      "episode  20 score -0.1 avg score -0.5\n",
      "episode  21 score -0.1 avg score -0.4\n",
      "episode  22 score -0.1 avg score -0.4\n",
      "episode  23 score -0.0 avg score -0.4\n",
      "episode  24 score -0.1 avg score -0.4\n",
      "episode  25 score -0.1 avg score -0.4\n",
      "episode  26 score -0.3 avg score -0.4\n",
      "episode  27 score -1.2 avg score -0.4\n",
      "episode  28 score -0.1 avg score -0.4\n",
      "episode  29 score -0.0 avg score -0.4\n",
      "episode  30 score -0.2 avg score -0.4\n",
      "episode  31 score -0.1 avg score -0.4\n",
      "episode  32 score -0.0 avg score -0.4\n",
      "episode  33 score -0.6 avg score -0.4\n",
      "episode  34 score -0.6 avg score -0.4\n",
      "episode  35 score -0.1 avg score -0.4\n",
      "episode  36 score -3.1 avg score -0.4\n",
      "episode  37 score -1.0 avg score -0.5\n",
      "episode  38 score -0.2 avg score -0.5\n",
      "episode  39 score -0.4 avg score -0.5\n",
      "episode  40 score -0.5 avg score -0.5\n",
      "episode  41 score -0.4 avg score -0.4\n",
      "episode  42 score -0.4 avg score -0.4\n",
      "episode  43 score -0.3 avg score -0.4\n",
      "episode  44 score -0.9 avg score -0.5\n",
      "episode  45 score -2.9 avg score -0.5\n",
      "episode  46 score 0.5 avg score -0.5\n",
      "episode  47 score -0.2 avg score -0.5\n",
      "episode  48 score -0.2 avg score -0.5\n",
      "episode  49 score -0.5 avg score -0.5\n",
      "episode  50 score -1.0 avg score -0.5\n",
      "episode  51 score -0.2 avg score -0.5\n",
      "episode  52 score -0.1 avg score -0.5\n",
      "episode  53 score 0.3 avg score -0.5\n",
      "episode  54 score -0.0 avg score -0.5\n",
      "episode  55 score -0.7 avg score -0.5\n",
      "episode  56 score -0.7 avg score -0.5\n",
      "episode  57 score -0.7 avg score -0.5\n",
      "episode  58 score -0.1 avg score -0.5\n",
      "episode  59 score -0.5 avg score -0.5\n",
      "episode  60 score 0.1 avg score -0.4\n",
      "episode  61 score -0.1 avg score -0.4\n",
      "episode  62 score -0.3 avg score -0.4\n",
      "episode  63 score -0.1 avg score -0.4\n",
      "episode  64 score -0.0 avg score -0.4\n",
      "episode  65 score -0.0 avg score -0.4\n",
      "episode  66 score -0.3 avg score -0.4\n",
      "episode  67 score -0.1 avg score -0.4\n",
      "episode  68 score 0.0 avg score -0.4\n",
      "episode  69 score -1.3 avg score -0.4\n",
      "episode  70 score -0.0 avg score -0.4\n",
      "episode  71 score -0.2 avg score -0.4\n",
      "episode  72 score -0.1 avg score -0.4\n",
      "episode  73 score -0.1 avg score -0.4\n",
      "episode  74 score -0.0 avg score -0.4\n",
      "episode  75 score -0.0 avg score -0.4\n",
      "episode  76 score -0.0 avg score -0.4\n",
      "episode  77 score -0.2 avg score -0.4\n",
      "episode  78 score 0.0 avg score -0.4\n",
      "episode  79 score -0.4 avg score -0.4\n",
      "episode  80 score -0.0 avg score -0.4\n",
      "episode  81 score -0.2 avg score -0.4\n",
      "episode  82 score 0.0 avg score -0.4\n",
      "episode  83 score -0.1 avg score -0.4\n",
      "episode  84 score -0.0 avg score -0.4\n",
      "episode  85 score -0.0 avg score -0.4\n",
      "episode  86 score 0.0 avg score -0.4\n",
      "episode  87 score 0.0 avg score -0.3\n",
      "episode  88 score -0.0 avg score -0.3\n",
      "episode  89 score -0.0 avg score -0.3\n",
      "episode  90 score 0.8 avg score -0.3\n",
      "episode  91 score -0.0 avg score -0.3\n",
      "episode  92 score -0.0 avg score -0.3\n",
      "episode  93 score 0.0 avg score -0.3\n",
      "episode  94 score -0.0 avg score -0.3\n",
      "episode  95 score 0.0 avg score -0.3\n",
      "episode  96 score 0.0 avg score -0.3\n",
      "episode  97 score 0.8 avg score -0.3\n",
      "episode  98 score -0.1 avg score -0.3\n",
      "episode  99 score -0.0 avg score -0.3\n",
      "episode  100 score -0.1 avg score -0.3\n",
      "episode  101 score -0.6 avg score -0.3\n",
      "episode  102 score 0.4 avg score -0.3\n",
      "episode  103 score -0.0 avg score -0.3\n",
      "episode  104 score -0.2 avg score -0.3\n",
      "episode  105 score 0.6 avg score -0.3\n",
      "episode  106 score 0.2 avg score -0.3\n",
      "episode  107 score -0.4 avg score -0.2\n",
      "episode  108 score -2.9 avg score -0.3\n",
      "episode  109 score 0.1 avg score -0.3\n",
      "episode  110 score -0.1 avg score -0.3\n",
      "episode  111 score -0.1 avg score -0.2\n",
      "episode  112 score -0.0 avg score -0.2\n",
      "episode  113 score -0.0 avg score -0.2\n",
      "episode  114 score 0.8 avg score -0.2\n",
      "episode  115 score -0.3 avg score -0.2\n",
      "episode  116 score -0.3 avg score -0.2\n",
      "episode  117 score -0.3 avg score -0.2\n",
      "episode  118 score -0.0 avg score -0.2\n",
      "episode  119 score -0.1 avg score -0.2\n",
      "episode  120 score -0.0 avg score -0.2\n",
      "episode  121 score -3.2 avg score -0.3\n",
      "episode  122 score 0.1 avg score -0.3\n",
      "episode  123 score -0.0 avg score -0.3\n",
      "episode  124 score -0.2 avg score -0.3\n",
      "episode  125 score -0.3 avg score -0.3\n",
      "episode  126 score -0.4 avg score -0.3\n",
      "episode  127 score 0.3 avg score -0.2\n",
      "episode  128 score -0.3 avg score -0.3\n",
      "episode  129 score -0.3 avg score -0.3\n",
      "episode  130 score -3.0 avg score -0.3\n",
      "episode  131 score -0.1 avg score -0.3\n",
      "episode  132 score 0.6 avg score -0.3\n",
      "episode  133 score -0.3 avg score -0.3\n",
      "episode  134 score -0.6 avg score -0.3\n",
      "episode  135 score -0.3 avg score -0.3\n",
      "episode  136 score 0.1 avg score -0.2\n",
      "episode  137 score -1.1 avg score -0.2\n",
      "episode  138 score -0.1 avg score -0.2\n",
      "episode  139 score 0.5 avg score -0.2\n",
      "episode  140 score 0.4 avg score -0.2\n",
      "episode  141 score -0.1 avg score -0.2\n",
      "episode  142 score -0.0 avg score -0.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_70505/2179043540.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mscore_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mavg_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_70505/3550537635.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mdiscount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Present step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Goes from current time step to end of the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mG_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mdiscount\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mG_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0mvar_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m       \u001b[0mpacked_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_strides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m     return strided_slice(\n\u001b[0m\u001b[1;32m   1042\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0mpacked_begin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1214\u001b[0;31m   op = gen_array_ops.strided_slice(\n\u001b[0m\u001b[1;32m   1215\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10503\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10504\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10505\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m  10506\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"StridedSlice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"begin_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10507\u001b[0m         \u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(alpha=0.0005, gamma =0.99, n_actions = env.action_space.n)\n",
    "#agent = Agent(alpha=0.05, gamma =0.9, n_actions = env.action_space.n)\n",
    "\n",
    "score_history = [] \n",
    "n_episodes = 1000\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    done = False\n",
    "    score = 0 \n",
    "    observation = format_obs(env.reset())\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        #print(action)   \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        observation_ = format_obs(observation_)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_ \n",
    "        score += reward \n",
    "    score_history.append(score)\n",
    "    \n",
    "    agent.learn()\n",
    "    \n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n",
    "    \n",
    "plt.plot(score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f420ea83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1264x336 at 0x32788A3A0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 1.0\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "done = False \n",
    "while not done:\n",
    "    display_screen(observation)\n",
    "    action = agent.choose_action(format_obs(observation))\n",
    "    observation_, reward, done, info = env.step(action)\n",
    "    print('reward',reward)\n",
    "    observation = observation_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6badc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:minihack]",
   "language": "python",
   "name": "conda-env-minihack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
