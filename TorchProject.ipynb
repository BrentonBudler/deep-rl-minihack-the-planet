{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "36124b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import minihack \n",
    "from nle import nethack \n",
    "\n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import flatten\n",
    "\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "d0cb0178",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3308420",
   "metadata": {},
   "source": [
    "## Model methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "84a2a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_screen(state):\n",
    "    screen = Image.fromarray(np.uint8(state['pixel']))\n",
    "    display(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "f56e16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma):\n",
    "    returns = []\n",
    "    r= 0\n",
    "    for reward in rewards[::-1]:\n",
    "        r = reward + gamma*r\n",
    "        returns.append(r)\n",
    "    returns.reverse()\n",
    "    returns = np.array(returns)\n",
    "    \n",
    "    if np.mean(returns)!= 0:\n",
    "         returns = returns - np.mean(returns)\n",
    "        \n",
    "    if np.std(returns) != 0:\n",
    "        returns = returns/ np.std(returns)\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "7b7d8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,s_size=8,h_size=128, a_size=4):\n",
    "        \n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.relu1= nn.ReLU()\n",
    "        \n",
    "        # To estimate the value function of the state \n",
    "        self.value_layer = nn.Linear(h_size, 1)\n",
    "        \n",
    "        # To calculate the probability of each action\n",
    "        self.action_layer = nn.Linear(h_size, a_size)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        #self.logprobs = []\n",
    "        #self.state_values = []\n",
    "        #self.rewards = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        state = self.fc1(state)\n",
    "        state = self.relu1(state)\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        action_probs = self.action_layer(state)\n",
    "        action_probs = self.softmax(action_probs)\n",
    "        \n",
    "        return action_probs,state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "5c6ba48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,s_size=8,h_size=128, a_size=4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels=1, out_channels=20,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = Conv2d(in_channels=20, out_channels=50,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=1600, out_features=500)\n",
    "        self.relu3 = ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = Linear(in_features=500, out_features=h_size)\n",
    "\n",
    "        \n",
    "        # To estimate the value function of the state \n",
    "        self.value_layer = nn.Linear(h_size, 1)\n",
    "\n",
    "        # To calculate the probability of each action\n",
    "        self.action_layer = nn.Linear(h_size, a_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "        #self.logprobs = []\n",
    "        #self.state_values = []\n",
    "        #self.rewards = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        x  = torch.from_numpy(state).float().to(device)\n",
    "\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => RELU => POOL layers\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # flatten the output from the previous layer and pass it\n",
    "        # through our only set of FC => RELU layers\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        state_value = self.value_layer(x)\n",
    "        \n",
    "        action_probs = self.action_layer(x)\n",
    "        action_probs = self.softmax(action_probs)\n",
    "        \n",
    "        return action_probs,state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "c08a0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, model, seed, learning_rate,betas, number_episodes, max_episode_length, gamma, max_reward, verbose=True):\n",
    "    # set random seeds (for reproducibility)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,betas=betas)\n",
    "    scores =[]\n",
    "    \n",
    "    for i in range(number_episodes):\n",
    "        # Reset environment\n",
    "        state = format_state(env.reset())\n",
    "        # Flag to see if episode has terminated\n",
    "        done = False\n",
    "        \n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        state_values = []\n",
    "        \n",
    "        for t in range(1, max_episode_length):\n",
    "            action_probs,state_value = model.forward(state)\n",
    "            \n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            next_state = format_state(next_state)\n",
    "    \n",
    "            rewards.append(reward)\n",
    "            log_probs.append(distribution.log_prob(action))\n",
    "            state_values.append(state_value)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores.append(np.sum(rewards))\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        returns = torch.from_numpy(returns).float().to(device)\n",
    "        print(\"Episode:\",i,\"Average Reward:\",np.mean(scores[-100:]))\n",
    "        if np.mean(scores[-100:])>= max_reward:\n",
    "            policy = model\n",
    "            return policy, scores\n",
    "\n",
    "        loss = 0\n",
    "        for logprob, value, reward in zip(log_probs, state_values, returns):\n",
    "            advantage = reward  - value.item()\n",
    "            action_loss = -logprob * advantage\n",
    "            #value = value.resize(len(value),1)\n",
    "            value_loss = F.smooth_l1_loss(value, reward)\n",
    "            loss += (action_loss + value_loss)   \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "    policy = model\n",
    "\n",
    "    return policy, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903aa6a",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "cd242727",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVIGATE_ACTIONS = (\n",
    "    nethack.CompassDirection.N,\n",
    "    nethack.CompassDirection.E,\n",
    "    nethack.CompassDirection.S,\n",
    "    nethack.CompassDirection.W,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "b8128402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maze_explore_reward(env, prev_obs, action, next_obs):\n",
    "    if (prev_obs[0] == 2359).sum() > (next_obs[0] == 2359).sum():\n",
    "        return 0.1\n",
    "    return 0\n",
    "\n",
    "def standing_still(env, prev_obs, action, next_obs):\n",
    "    if (prev_obs[13] == next_obs[13]).all():\n",
    "        return -0.1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "31eb875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihack import RewardManager\n",
    "reward_gen = RewardManager()\n",
    "reward_gen.add_eat_event(\"apple\", reward=1)\n",
    "reward_gen.add_custom_reward_fn(maze_explore_reward)\n",
    "#reward_gen.add_custom_reward_fn(standing_still)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "17f4026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-MazeWalk-9x9-v0\", observation_keys=[\"glyphs\",\"pixel\"], actions=NAVIGATE_ACTIONS, \n",
    "               max_episode_steps=10000)\n",
    "              #reward_manager= reward_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "133ab75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "2c860bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1264x336 at 0x172966FA0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_screen(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "34deab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(state):\n",
    "    # Flatten the 2D array \n",
    "    #glyphs = state[\"glyphs\"].flatten()\n",
    "    glyphs = state[\"glyphs\"]\n",
    "    # Normalize\n",
    "    glyphs = glyphs/glyphs.max()\n",
    "    glyphs = glyphs.reshape((1,1,21,79))\n",
    "    return glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "59d2d3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 21, 79)"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_state(env.reset()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b3bec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward -1.73 Average Reward: -1.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_90569/3403732991.py:52: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.smooth_l1_loss(value, reward)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Reward -99.97 Average Reward: -50.85\n",
      "Episode: 2 Reward -99.97 Average Reward: -67.22333333333334\n",
      "Episode: 3 Reward 0.91 Average Reward: -50.190000000000005\n",
      "Episode: 4 Reward 0.64 Average Reward: -40.02400000000001\n",
      "Episode: 5 Reward 0.1200000000000001 Average Reward: -33.333333333333336\n",
      "Episode: 6 Reward -0.8800000000000001 Average Reward: -28.69714285714286\n",
      "Episode: 7 Reward 0.83 Average Reward: -25.006249999999998\n",
      "Episode: 8 Reward 0.019999999999999962 Average Reward: -22.22555555555555\n",
      "Episode: 9 Reward -6.35 Average Reward: -20.637999999999998\n",
      "Episode: 10 Reward -9.23 Average Reward: -19.600909090909088\n",
      "Episode: 11 Reward -8.97 Average Reward: -18.714999999999996\n",
      "Episode: 12 Reward -1.04 Average Reward: -17.35538461538461\n",
      "Episode: 13 Reward 0.91 Average Reward: -16.05071428571428\n",
      "Episode: 14 Reward 0.0699999999999999 Average Reward: -14.975999999999997\n",
      "Episode: 15 Reward -10.82 Average Reward: -14.716249999999999\n",
      "Episode: 16 Reward -0.33000000000000007 Average Reward: -13.87\n",
      "Episode: 17 Reward -8.040000000000001 Average Reward: -13.54611111111111\n",
      "Episode: 18 Reward -9.49 Average Reward: -13.332631578947368\n",
      "Episode: 19 Reward 0.7099999999999999 Average Reward: -12.6305\n",
      "Episode: 20 Reward 0.88 Average Reward: -11.987142857142857\n",
      "Episode: 21 Reward -0.5800000000000001 Average Reward: -11.468636363636364\n",
      "Episode: 22 Reward -12.040000000000001 Average Reward: -11.493478260869566\n",
      "Episode: 23 Reward -23.54 Average Reward: -11.995416666666666\n",
      "Episode: 24 Reward -28.189999999999998 Average Reward: -12.6432\n",
      "Episode: 25 Reward -8.32 Average Reward: -12.476923076923075\n",
      "Episode: 26 Reward 0.85 Average Reward: -11.983333333333333\n",
      "Episode: 27 Reward -6.54 Average Reward: -11.78892857142857\n",
      "Episode: 28 Reward 0.91 Average Reward: -11.35103448275862\n",
      "Episode: 29 Reward -19.22 Average Reward: -11.613333333333333\n",
      "Episode: 30 Reward -3.35 Average Reward: -11.346774193548388\n",
      "Episode: 31 Reward -6.720000000000001 Average Reward: -11.202187499999999\n",
      "Episode: 32 Reward -9.3 Average Reward: -11.144545454545455\n",
      "Episode: 33 Reward -99.93 Average Reward: -13.755882352941176\n",
      "Episode: 34 Reward -6.57 Average Reward: -13.550571428571429\n",
      "Episode: 35 Reward 0.85 Average Reward: -13.150555555555554\n",
      "Episode: 36 Reward -4.31 Average Reward: -12.91162162162162\n",
      "Episode: 37 Reward 0.96 Average Reward: -12.54657894736842\n",
      "Episode: 38 Reward -0.9300000000000002 Average Reward: -12.248717948717948\n",
      "Episode: 39 Reward 0.89 Average Reward: -11.92025\n",
      "Episode: 40 Reward -1.1800000000000002 Average Reward: -11.658292682926827\n",
      "Episode: 41 Reward -1.2 Average Reward: -11.409285714285712\n",
      "Episode: 42 Reward -16.980000000000004 Average Reward: -11.538837209302324\n",
      "Episode: 43 Reward 0.29000000000000004 Average Reward: -11.269999999999998\n",
      "Episode: 44 Reward 0.3600000000000001 Average Reward: -11.011555555555553\n",
      "Episode: 45 Reward 0.85 Average Reward: -10.753695652173912\n",
      "Episode: 46 Reward -5.57 Average Reward: -10.643404255319147\n",
      "Episode: 47 Reward -2.0300000000000002 Average Reward: -10.463958333333332\n",
      "Episode: 48 Reward -4.66 Average Reward: -10.345510204081632\n",
      "Episode: 49 Reward -15.790000000000003 Average Reward: -10.4544\n",
      "Episode: 50 Reward -5.17 Average Reward: -10.35078431372549\n",
      "Episode: 51 Reward 0.94 Average Reward: -10.133653846153845\n",
      "Episode: 52 Reward 0.88 Average Reward: -9.925849056603772\n",
      "Episode: 53 Reward 0.6799999999999999 Average Reward: -9.729444444444445\n",
      "Episode: 54 Reward -37.980000000000004 Average Reward: -10.24309090909091\n",
      "Episode: 55 Reward 0.78 Average Reward: -10.04625\n",
      "Episode: 56 Reward 0.05999999999999994 Average Reward: -9.868947368421054\n",
      "Episode: 57 Reward -13.29 Average Reward: -9.927931034482759\n",
      "Episode: 58 Reward -7.29 Average Reward: -9.883220338983051\n",
      "Episode: 59 Reward -0.75 Average Reward: -9.731\n",
      "Episode: 60 Reward -17.27 Average Reward: -9.854590163934427\n",
      "Episode: 61 Reward -16.150000000000002 Average Reward: -9.956129032258064\n",
      "Episode: 62 Reward -0.20000000000000007 Average Reward: -9.801269841269841\n",
      "Episode: 63 Reward 0.97 Average Reward: -9.63296875\n",
      "Episode: 64 Reward -3.49 Average Reward: -9.538461538461538\n",
      "Episode: 65 Reward -6.16 Average Reward: -9.487272727272726\n",
      "Episode: 66 Reward 0.6599999999999999 Average Reward: -9.335820895522389\n",
      "Episode: 67 Reward -0.15999999999999998 Average Reward: -9.200882352941177\n",
      "Episode: 68 Reward -6.92 Average Reward: -9.16782608695652\n",
      "Episode: 69 Reward -0.6900000000000002 Average Reward: -9.046714285714286\n",
      "Episode: 70 Reward -1.27 Average Reward: -8.937183098591548\n",
      "Episode: 71 Reward -5.9 Average Reward: -8.895000000000001\n",
      "Episode: 72 Reward 0.56 Average Reward: -8.765479452054796\n",
      "Episode: 73 Reward -4.9 Average Reward: -8.713243243243245\n",
      "Episode: 74 Reward -1.25 Average Reward: -8.613733333333334\n",
      "Episode: 75 Reward -15.450000000000001 Average Reward: -8.703684210526317\n",
      "Episode: 76 Reward -0.6000000000000001 Average Reward: -8.59844155844156\n",
      "Episode: 77 Reward -2.93 Average Reward: -8.525769230769232\n",
      "Episode: 78 Reward -14.81 Average Reward: -8.605316455696203\n",
      "Episode: 79 Reward -2.0900000000000003 Average Reward: -8.523875\n",
      "Episode: 80 Reward 0.43999999999999995 Average Reward: -8.413209876543208\n",
      "Episode: 81 Reward 0.72 Average Reward: -8.301829268292682\n",
      "Episode: 82 Reward -3.39 Average Reward: -8.242650602409636\n",
      "Episode: 83 Reward -0.6000000000000002 Average Reward: -8.151666666666666\n",
      "Episode: 84 Reward -0.3200000000000001 Average Reward: -8.059529411764705\n",
      "Episode: 85 Reward 0.9299999999999999 Average Reward: -7.955\n",
      "Episode: 86 Reward -10.940000000000001 Average Reward: -7.989310344827587\n",
      "Episode: 87 Reward -15.63 Average Reward: -8.076136363636364\n",
      "Episode: 88 Reward -3.12 Average Reward: -8.020449438202247\n",
      "Episode: 89 Reward -10.559999999999999 Average Reward: -8.048666666666666\n",
      "Episode: 90 Reward -3.81 Average Reward: -8.00208791208791\n",
      "Episode: 91 Reward -9.809999999999999 Average Reward: -8.02173913043478\n",
      "Episode: 92 Reward -4.77 Average Reward: -7.986774193548384\n",
      "Episode: 93 Reward 0.84 Average Reward: -7.892872340425529\n",
      "Episode: 94 Reward -1.9700000000000002 Average Reward: -7.830526315789471\n",
      "Episode: 95 Reward -0.66 Average Reward: -7.7558333333333325\n",
      "Episode: 96 Reward 0.93 Average Reward: -7.666288659793814\n",
      "Episode: 97 Reward 0.49 Average Reward: -7.5830612244897955\n",
      "Episode: 98 Reward -0.43000000000000005 Average Reward: -7.51080808080808\n",
      "Episode: 99 Reward 0.96 Average Reward: -7.426099999999999\n",
      "Episode: 100 Reward -2.1100000000000003 Average Reward: -7.429899999999999\n",
      "Episode: 101 Reward 0.94 Average Reward: -6.420799999999998\n",
      "Episode: 102 Reward 0.73 Average Reward: -5.4138\n",
      "Episode: 103 Reward -3.25 Average Reward: -5.4554\n",
      "Episode: 104 Reward -8.42 Average Reward: -5.545999999999999\n",
      "Episode: 105 Reward -9.72 Average Reward: -5.644399999999999\n",
      "Episode: 106 Reward 0.63 Average Reward: -5.629300000000001\n",
      "Episode: 107 Reward -4.09 Average Reward: -5.6785000000000005\n",
      "Episode: 108 Reward 0.84 Average Reward: -5.6703\n",
      "Episode: 109 Reward -1.8000000000000003 Average Reward: -5.624799999999999\n",
      "Episode: 110 Reward -11.0 Average Reward: -5.6425\n",
      "Episode: 111 Reward -0.1399999999999999 Average Reward: -5.554199999999998\n",
      "Episode: 112 Reward -1.4500000000000002 Average Reward: -5.558299999999999\n",
      "Episode: 113 Reward -5.58 Average Reward: -5.623200000000001\n",
      "Episode: 114 Reward 0.98 Average Reward: -5.6141000000000005\n",
      "Episode: 115 Reward 0.91 Average Reward: -5.4968\n",
      "Episode: 116 Reward 0.92 Average Reward: -5.484300000000001\n",
      "Episode: 117 Reward -3.99 Average Reward: -5.443800000000001\n",
      "Episode: 118 Reward 0.87 Average Reward: -5.340200000000001\n",
      "Episode: 119 Reward -0.26 Average Reward: -5.3499\n",
      "Episode: 120 Reward -3.5199999999999996 Average Reward: -5.3938999999999995\n",
      "Episode: 121 Reward 0.6 Average Reward: -5.3821\n",
      "Episode: 122 Reward -6.67 Average Reward: -5.328399999999999\n",
      "Episode: 123 Reward 0.08000000000000002 Average Reward: -5.0922\n",
      "Episode: 124 Reward 0.79 Average Reward: -4.8024000000000004\n",
      "Episode: 125 Reward 0.7 Average Reward: -4.712200000000001\n",
      "Episode: 126 Reward -0.06000000000000011 Average Reward: -4.721300000000001\n",
      "Episode: 127 Reward -0.20999999999999996 Average Reward: -4.658\n",
      "Episode: 128 Reward 1.0 Average Reward: -4.657100000000001\n",
      "Episode: 129 Reward -0.52 Average Reward: -4.4701\n",
      "Episode: 130 Reward 0.36 Average Reward: -4.433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 131 Reward -2.9900000000000007 Average Reward: -4.395700000000001\n",
      "Episode: 132 Reward 0.77 Average Reward: -4.295000000000001\n",
      "Episode: 133 Reward 0.6599999999999999 Average Reward: -3.2891000000000004\n",
      "Episode: 134 Reward 0.71 Average Reward: -3.2163000000000004\n",
      "Episode: 135 Reward -1.33 Average Reward: -3.2381\n",
      "Episode: 136 Reward 0.6699999999999999 Average Reward: -3.1883\n",
      "Episode: 137 Reward -0.29000000000000015 Average Reward: -3.2008000000000005\n",
      "Episode: 138 Reward -12.33 Average Reward: -3.3148\n"
     ]
    }
   ],
   "source": [
    "model = ActorCritic(h_size=512, a_size=env.action_space.n)\n",
    "policy, scores = actor_critic(env=env, model= model, seed=42, learning_rate=0.02, betas=(0.9, 0.999),\n",
    "                           number_episodes=1500, max_episode_length=10000, gamma=0.99 ,verbose=True, max_reward=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(h_size=512, a_size=env.action_space.n)\n",
    "policy, scores = actor_critic(env=env, model= model, seed=42, learning_rate=0.02, betas=(0.9, 0.999),\n",
    "                           number_episodes=1500, max_episode_length=10000, gamma=0.99 ,verbose=True, max_reward=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:minihack] *",
   "language": "python",
   "name": "conda-env-minihack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
