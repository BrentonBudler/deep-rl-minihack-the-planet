{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdffc895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from nle import nethack\n",
    "import gym\n",
    "import minihack\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nle import nethack\n",
    "import time\n",
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552697cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d040060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(keras.Model):\n",
    "    \n",
    "    # Initialize network and archtitecture\n",
    "    \n",
    "    #ADD CO-ORDS \"tty_cursor\"\n",
    "    def __init__(self, n_actions, fc1_dims=256, fc2_dims=128, fc3_dims = 256, fc4_dims=128, fc5_dims=64):\n",
    "        super(PolicyGradientNetwork,self).__init__()\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims \n",
    "        self.fc3_dims = fc3_dims\n",
    "        self.fc4_dims = fc4_dims \n",
    "        self.fc5_dims = fc5_dims \n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        \n",
    "        # Conv for full glyphs \n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool1 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.conv2 = Conv2D(filters=16, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool2 =MaxPool2D(pool_size=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        \n",
    "        # Conv for cropped glyphs \n",
    "        self.conv3= Conv2D(filters=32, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool3 = MaxPool2D(pool_size=(2, 2))\n",
    "        self.conv4 = Conv2D(filters=16, kernel_size=(2,2) , padding = 'same', activation='relu')\n",
    "        self.maxpool4 =MaxPool2D(pool_size=(2, 2))\n",
    "        self.flatten_crop= Flatten()\n",
    "        \n",
    "\n",
    "        self.fc1 = Dense(self.fc1_dims, activation = 'relu')\n",
    "        self.fc2 = Dense(self.fc2_dims, activation = 'relu')\n",
    "        self.fc3 = Dense(self.fc3_dims, activation = 'relu')\n",
    "        self.fc4 = Dense(self.fc4_dims, activation = 'relu')\n",
    "        self.fc5 = Dense(self.fc5_dims, activation = 'relu')\n",
    "        \n",
    "        \n",
    "        self.pi = Dense(self.n_actions, activation = 'softmax')\n",
    "    \n",
    "    # Forward Pass\n",
    "    def call(self,state):\n",
    "        \n",
    "        glyphs_t = tf.convert_to_tensor([state[\"glyphs\"]], dtype=tf.float32)\n",
    "        glyphs_cropped_t = tf.convert_to_tensor([state[\"glyphs_crop\"]], dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        stats_t = tf.convert_to_tensor([state[\"stats\"]], dtype=tf.float32)\n",
    "        inv_glyphs_t = tf.convert_to_tensor([state[\"inv_glyphs\"]], dtype=tf.float32)\n",
    "        stats_inv_t = tf.concat([stats_t,inv_glyphs_t], 1)\n",
    "        \n",
    "        message_t = tf.convert_to_tensor([state[\"message\"]], dtype=tf.float32)\n",
    "        stats_inv_mess_t = tf.concat([stats_inv_t,message_t], 1)\n",
    "        \n",
    "        # Full\n",
    "        conv_value = self.conv1(glyphs_t)\n",
    "        conv_value = self.maxpool1(conv_value)\n",
    "        conv_value = self.conv2(conv_value)\n",
    "        conv_value = self.maxpool2(conv_value)\n",
    "        conv_value = self.flatten(conv_value)\n",
    "        \n",
    "        # Cropped \n",
    "        conv_value_crop = self.conv3(glyphs_cropped_t)\n",
    "        conv_value_crop = self.maxpool3(conv_value_crop)\n",
    "        conv_value_crop = self.conv4(conv_value_crop)\n",
    "        conv_value_crop = self.maxpool4(conv_value_crop)\n",
    "        conv_value_crop = self.flatten_crop(conv_value_crop)\n",
    "        \n",
    "        # Inclusion of Stats, Inv_glpyhs, Message \n",
    "        fc_value = self.fc1(stats_inv_mess_t)\n",
    "        fc_value = self.fc2(fc_value)\n",
    "        \n",
    "        value = tf.concat([conv_value,conv_value_crop, fc_value],1)\n",
    "\n",
    "        value = self.fc3(value)\n",
    "        value = self.fc4(value)\n",
    "        value = self.fc5(value)\n",
    "        \n",
    "        pi = self.pi(value)\n",
    "        \n",
    "        return pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c6cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, alpha = 0.003, gamma = 0.99, n_actions = 4):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Keep tracks of the actions we take, the states we visit and the rewards we get \n",
    "        self.state_memory = [] \n",
    "        self.action_memory = [] \n",
    "        self.reward_memory = [] \n",
    "        \n",
    "        # Policy Network \n",
    "        self.policy = PolicyGradientNetwork(n_actions=n_actions)\n",
    "        self.policy.compile(optimizer=Adam(learning_rate=self.alpha))\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        # Convert numpy array to tensorflow tensor\n",
    "        #state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "        state = observation\n",
    "        \n",
    "        # Then we want to pass that state through the policy network \n",
    "        # Which gives us the probability distribution for choosing each action \n",
    "        probs = self.policy(state)\n",
    "        \n",
    "        # Used when you have a set of discrete classes and you want to choose one of classes with a particular proba\n",
    "        action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "        \n",
    "        action = action_probs.sample()\n",
    "\n",
    "        return action.numpy()[0]\n",
    "    \n",
    "    def store_transition(self, observation, action, reward):\n",
    "        # Stores the transitions of the current episode \n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    def learn(self):\n",
    "        # Now we need a function to actually perform the agents learning \n",
    "        \n",
    "        # Convert actions and rewards to tensors\n",
    "        actions = tf.convert_to_tensor(self.action_memory, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(self.reward_memory, dtype=tf.float32)\n",
    "        \n",
    "        # Calculate sum of future rewards which follow each time step \n",
    "        G = np.zeros_like(rewards)\n",
    "        # Now we iterate over the agents memory \n",
    "        for t in range(len(rewards)):\n",
    "            G_sum = 0 \n",
    "            discount = 1 # Present step \n",
    "            for k in range(t, len(rewards)): # Goes from current time step to end of the episode \n",
    "                G_sum += rewards[k]*discount\n",
    "                discount *= self.gamma\n",
    "            G[t]= G_sum \n",
    "        \n",
    "        # Now we can calculate the gradient at each time step in the episode \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 0 \n",
    "            # Now workign back through the agent's memory \n",
    "            for idx, (g,state) in enumerate(zip(G,self.state_memory)):\n",
    "                #state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "                probs = self.policy(state)\n",
    "                action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "                # Log probability for the gradient calculation\n",
    "                log_prob = action_probs.log_prob(actions[idx])\n",
    "                loss += -g * tf.squeeze(log_prob)\n",
    "                \n",
    "        gradient = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        #self.policy.optimizer.apply_gradients(zip(gradient, self.policy.trainable_variables))\n",
    "        self.policy.optimizer.apply_gradients((grad, var) for (grad, var) in zip(gradient, self.policy.trainable_variables) if grad is not None)\n",
    "        \n",
    "        # Now at the end of every episode we need to zero out (empty) our agent's memory \n",
    "        \n",
    "        self.state_memory = [] \n",
    "        self.action_memory = [] \n",
    "        self.reward_memory = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9520c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_obs(observation):\n",
    "    glyphs = np.array(observation[\"glyphs\"]/observation[\"glyphs\"].max())\n",
    "    glyphs = glyphs.reshape((21,79,1))\n",
    "    \n",
    "    glyphs_crop = np.array(observation[\"glyphs_crop\"]/observation[\"glyphs_crop\"].max())\n",
    "    glyphs_crop = glyphs_crop.reshape((9,9,1))\n",
    "    \n",
    "    bl_max = observation[\"blstats\"].max()\n",
    "    if bl_max == 0:\n",
    "        stats = np.array(observation[\"blstats\"])\n",
    "    else:\n",
    "        stats = np.array(observation[\"blstats\"]/bl_max)\n",
    "        \n",
    "    \n",
    "    inv_glyphs = np.array(observation[\"inv_glyphs\"]/observation[\"inv_glyphs\"].max())\n",
    "    \n",
    "    mes_max = observation[\"message\"].max()\n",
    "    if mes_max == 0:\n",
    "        message = np.array(observation[\"message\"])\n",
    "    else:\n",
    "        message = np.array(observation[\"message\"]/mes_max)\n",
    "    \n",
    "    \n",
    "    observation = {'glyphs':glyphs,'glyphs_crop':glyphs_crop, 'stats':stats,'inv_glyphs':inv_glyphs, 'message': message} \n",
    "    \n",
    "    return observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4754a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVIGATE_ACTIONS = (\n",
    "    nethack.CompassDirection.N,\n",
    "    nethack.CompassDirection.E,\n",
    "    nethack.CompassDirection.S,\n",
    "    nethack.CompassDirection.W,\n",
    "#     nethack.CompassDirection.NW,\n",
    "#     nethack.Command.PICKUP,\n",
    "#     nethack.Command.APPLY,\n",
    "#     nethack.Command.FIRE,\n",
    "#     nethack.Command.RUSH,\n",
    "#     nethack.Command.ZAP, \n",
    "#     nethack.Command.PUTON,\n",
    "#     nethack.Command.READ, \n",
    "#     nethack.Command.WEAR, \n",
    "#     nethack.Command.QUAFF\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d4fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages(env, prev_obs, action, next_obs):\n",
    "    print(\"messss: \",decode_utf(next_obs[5]))\n",
    "    if \"f - \" in decode_utf(next_obs[5]) or  \"g - \" in decode_utf(next_obs[5]):\n",
    "        return 1\n",
    "    if \"[\" in decode_utf(next_obs[5]) and  \"]\" in decode_utf(next_obs[5]):\n",
    "        return 2\n",
    "    if \"The lava cools and solidifies\" in decode_utf(next_obs[5]):\n",
    "        return 3\n",
    "#     if \"In what direction?\" in decode_utf(next_obs[5]):\n",
    "#         return 2\n",
    "    if \"You start to float in the air!\" in decode_utf(next_obs[5]):\n",
    "        return 3\n",
    "    if \"You finish your dressing maneuver.\" in decode_utf(next_obs[5]):\n",
    "        return 3\n",
    "    return -0.01\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce9afdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maze_explore_reward(env, prev_obs, action, next_obs):\n",
    "    if (prev_obs[0] == 2359).sum() > (next_obs[0] == 2359).sum():\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119dc607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standing_still(env, prev_obs, action, next_obs):\n",
    "    if (prev_obs[13] == next_obs[13]).all():\n",
    "        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eefc45d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decode_utf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_80976/2831677860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[100:225,300:900])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_utf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#env.reward_manager.add_location_event(\"staircase down\", reward=1.1, terminal_sufficient = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decode_utf' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x2160 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from minihack import RewardManager\n",
    "reward_gen = RewardManager()\n",
    "\n",
    "reward_gen.add_eat_event(\"apple\", reward=1)\n",
    "reward_gen.add_coordinate_event((11,28), reward = 10000, terminal_sufficient = True) # first door\n",
    "reward_gen.add_custom_reward_fn(maze_explore_reward)\n",
    "reward_gen.add_custom_reward_fn(standing_still)\n",
    "\n",
    "# reward_gen.add_coordinate_event((11,38), reward = 1, terminal_required = False)\n",
    "# reward_gen.kill_event(\"minotaur\", reward = 2, terminal_required = False)\n",
    "\n",
    "env = gym.make(\"MiniHack-Quest-Hard-v0\", observation_keys=[\"glyphs\",\"glyphs_crop\",\"pixel\",\"blstats\",\"inv_glyphs\",\"message\", \"screen_descriptions\", \"tty_cursor\"],\n",
    "              actions  = NAVIGATE_ACTIONS, reward_manager = reward_gen)\n",
    "\n",
    "state = env.reset()\n",
    "plt.figure(figsize=(15, 30))\n",
    "plt.imshow(state['pixel'])#[100:225,300:900])\n",
    "print(decode_utf(state['message']))\n",
    "#env.reward_manager.add_location_event(\"staircase down\", reward=1.1, terminal_sufficient = True)\n",
    "\n",
    "\n",
    "# if(env.screen_contains(\"potion\")):\n",
    "#     env.reward_manager.add_location_event(\"potion\", reward=1.23, terminal_required=False)\n",
    "# if(env.screen_contains(\"horn\")):\n",
    "#     env.reward_manager.add_location_event(\"horn\", reward=3, terminal_required=False)\n",
    "# if(env.screen_contains(\"ring\")):\n",
    "#     env.reward_manager.add_location_event(\"ring\", reward=3, terminal_required=False)\n",
    "# if(env.screen_contains(\"wand\")):\n",
    "#     env.reward_manager.add_location_event(\"wand\", reward=3, terminal_required=False)\n",
    "# if(env.screen_contains(\"boots\")):\n",
    "#     env.reward_manager.add_location_event(\"boots\", reward=3, terminal_required=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68d8dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(1)\n",
    "plt.figure(figsize=(15, 30))\n",
    "plt.imshow(state['pixel'])#[100:225,300:900])\n",
    "print(decode_utf(state['message']))\n",
    "print(reward)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfe2db03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_80976/3269927882.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_utf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'decoded' is not defined"
     ]
    }
   ],
   "source": [
    "def decode_utf(arr):\n",
    "    return array.array('b', arr).tobytes().decode('utf-8')\n",
    "\n",
    "sd = state[\"screen_descriptions\"]\n",
    "for i in range(sd.shape[0]):\n",
    "    for j in range(sd.shape[1]):\n",
    "        if sd[i,j].max()!=0:\n",
    "            decoded = decode_utf(sd[i,j])\n",
    "        print(i,j, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56bdab76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2359"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[\"glyphs\"][11,38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b85ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 CompassDirection.N\n",
      "1 CompassDirection.E\n",
      "2 CompassDirection.S\n",
      "3 CompassDirection.W\n"
     ]
    }
   ],
   "source": [
    "for i in range(env.action_space.n):\n",
    "    print(i, env._actions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df122d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(alpha=0.0005, gamma =0.99, n_actions = env.action_space.n)\n",
    "#agent = Agent(alpha=0.05, gamma =0.9, n_actions = env.action_space.n)\n",
    "\n",
    "score_history = [] \n",
    "n_episodes = 1000\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    done = False\n",
    "    score = 0 \n",
    "    observation = format_obs(env.reset())\n",
    "    \n",
    "    for j in range(250):\n",
    "        action = agent.choose_action(observation)  \n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        observation_ = format_obs(observation_)\n",
    "        agent.store_transition(observation, action, reward)\n",
    "        observation = observation_ \n",
    "        score += reward \n",
    "        if done:\n",
    "            break\n",
    "    score_history.append(score)\n",
    "    \n",
    "    agent.learn()\n",
    "    \n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    print('episode ', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n",
    "    \n",
    "plt.plot(score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796d24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e6ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:minihack] *",
   "language": "python",
   "name": "conda-env-minihack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
