{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "1b1c61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import minihack \n",
    "from nle import nethack \n",
    "\n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import flatten\n",
    "\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "f9432ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "6999b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maze_explore_reward(env, prev_obs, action, next_obs):\n",
    "    if (prev_obs[0] == 2359).sum() > (next_obs[0] == 2359).sum():\n",
    "        return 0.1\n",
    "    return 0\n",
    "\n",
    "def standing_still(env, prev_obs, action, next_obs):\n",
    "    if (prev_obs[13] == next_obs[13]).all():\n",
    "        return -0.1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce189e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "617a5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minihack import RewardManager\n",
    "reward_gen = RewardManager()\n",
    "reward_gen.add_eat_event(\"apple\", reward=1)\n",
    "#reward_gen.add_location_event(\"staircase down\", reward=10,terminal_required=True,terminal_sufficient=False, repeatable=False)\n",
    "reward_gen.add_custom_reward_fn(maze_explore_reward)\n",
    "reward_gen.add_custom_reward_fn(standing_still)\n",
    "\n",
    "\n",
    "#You get it if it is not on the screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "f7b146bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_screen(state):\n",
    "    screen = Image.fromarray(np.uint8(state['pixel']))\n",
    "    display(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ae7e81b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MiniHack-MazeWalk-9x9-v0\", observation_keys=[\"glyphs\",\"pixel\",\"tty_cursor\"],\n",
    "               max_episode_steps=10000,penalty_step=-5)\n",
    "\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "3697504f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1264x336 at 0x15F815340>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_screen(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "4021347c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1264x336 at 0x1690906D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False 0.0\n"
     ]
    }
   ],
   "source": [
    "next_state,reward,done,_ = env.step(2)\n",
    "#loc = next_state[\"tty_cursor\"]\n",
    "\n",
    "display_screen(next_state)\n",
    "print(done,reward)\n",
    "#env.get_screen_description(x=loc[1],y=loc[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411e6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "f46aa52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'staircase up', 'floor of a room']\n",
      "['', 'human rogue called Agent', '', '']\n",
      "['', 'floor of a room', '']\n"
     ]
    }
   ],
   "source": [
    "print(env.get_neighbor_descriptions()[:3])\n",
    "print(env.get_neighbor_descriptions()[3:7])\n",
    "print(env.get_neighbor_descriptions()[6:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "c4292df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c8aa9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,s_size=8,h_size=128, a_size=4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels=1, out_channels=20,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = Conv2d(in_channels=20, out_channels=50,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=1600, out_features=500)\n",
    "        self.relu3 = ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = Linear(in_features=500, out_features=h_size)\n",
    "\n",
    "        \n",
    "        # To estimate the value function of the state \n",
    "        self.value_layer = nn.Linear(h_size, 1)\n",
    "\n",
    "        # To calculate the probability of each action\n",
    "        self.action_layer = nn.Linear(h_size, a_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        \n",
    "        #self.logprobs = []\n",
    "        #self.state_values = []\n",
    "        #self.rewards = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        x  = torch.from_numpy(state).float().to(device)\n",
    "\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => RELU => POOL layers\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # flatten the output from the previous layer and pass it\n",
    "        # through our only set of FC => RELU layers\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        state_value = self.value_layer(x)\n",
    "        \n",
    "        action_probs = self.action_layer(x)\n",
    "        action_probs = self.softmax(action_probs)\n",
    "        \n",
    "        return action_probs,state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "0d5a0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_screen(state):\n",
    "    screen = Image.fromarray(np.uint8(state['pixel']))\n",
    "    display(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1a35c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma):\n",
    "    returns = []\n",
    "    r= 0\n",
    "    for reward in rewards[::-1]:\n",
    "        r = reward + gamma*r\n",
    "        returns.append(r)\n",
    "    returns.reverse()\n",
    "    returns = np.array(returns)\n",
    "    \n",
    "    if np.mean(returns)!= 0:\n",
    "         returns = returns - np.mean(returns)\n",
    "        \n",
    "    if np.std(returns) != 0:\n",
    "        returns = returns/ np.std(returns)\n",
    "\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8feee086",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim,a_size, hidden_dim=64, layer_dim=3, output_dim=1, dropout_prob=0.2, h_size=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels=1, out_channels=20,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = Conv2d(in_channels=20, out_channels=50,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.fc1 = Linear(in_features=1600, out_features=500)\n",
    "        self.relu3 = ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = Linear(in_features=500, out_features=h_size)\n",
    "\n",
    "        \n",
    "#         # To estimate the value function of the state \n",
    "#         self.value_layer = nn.Linear(h_size, 1)\n",
    "\n",
    "#         # To calculate the probability of each action\n",
    "#         self.action_layer = nn.Linear(h_size, a_size)\n",
    "\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fcfinal = nn.Linear(hidden_dim, h_size)    \n",
    "        \n",
    "        # To estimate the value function of the state \n",
    "        self.value_layer = nn.Linear(h_size, 1)\n",
    "        \n",
    "        # To calculate the probability of each action\n",
    "        self.action_layer = nn.Linear(h_size, a_size)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        #self.logprobs = []\n",
    "        #self.state_values = []\n",
    "        #self.rewards = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        x  = torch.from_numpy(state).float().to(device)\n",
    "\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => RELU => POOL layers\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # flatten the output from the previous layer and pass it\n",
    "        # through our only set of FC => RELU layers\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #x = x.reshape((1,-1,len(x)))\n",
    "\n",
    "        x = torch.reshape(x, (1,1,x.shape[1]))\n",
    "        #print(x.shape) # SOMEHOW WRONG\n",
    "\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(1), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initializing cell state for first input with zeros\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(1), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        # Forward propagation by passing in the input, hidden state, and cell state into the model\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fcfinal(out)\n",
    "        \n",
    "        state_value = self.value_layer(out)\n",
    "        \n",
    "        action_probs = self.action_layer(out)\n",
    "        action_probs = self.softmax(action_probs)\n",
    "        \n",
    "        return action_probs,state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "bc846f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_critic(env, model, seed, learning_rate,betas, number_episodes, max_episode_length, gamma, max_reward, verbose=True):\n",
    "    # set random seeds (for reproducibility)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,betas=betas)\n",
    "    scores =[]\n",
    "    \n",
    "    for i in range(number_episodes):\n",
    "        # Reset environment\n",
    "        state = format_state(env.reset())\n",
    "        # Flag to see if episode has terminated\n",
    "        done = False\n",
    "        \n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        state_values = []\n",
    "        \n",
    "        for t in range(1, max_episode_length):\n",
    "            \n",
    "            action_probs,state_value = model.forward(state)\n",
    "            \n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            next_state = format_state(next_state)\n",
    "    \n",
    "            rewards.append(reward)\n",
    "            log_probs.append(distribution.log_prob(action))\n",
    "            state_values.append(state_value)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores.append(np.sum(rewards))\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        returns = torch.from_numpy(returns).float().to(device)\n",
    "        print(\"Episode:\",i,\"Reward:\",np.sum(rewards),\"Average Reward:\",np.mean(scores[-100:]),\"Steps\",t)\n",
    "#         if np.mean(scores[-100:])>= max_reward:\n",
    "#             policy = model\n",
    "#             return policy, scores\n",
    "\n",
    "        loss = 0\n",
    "        for logprob, value, reward in zip(log_probs, state_values, returns):\n",
    "            advantage = reward  - value.item()\n",
    "            action_loss = -logprob * advantage\n",
    "            #value = value.resize(len(value),1)\n",
    "            value_loss = F.smooth_l1_loss(value, reward)\n",
    "            loss += (action_loss + value_loss)   \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "    policy = model\n",
    "\n",
    "    return policy, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265475ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae524b97",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c82919b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVIGATE_ACTIONS = (\n",
    "    nethack.CompassDirection.N,\n",
    "    nethack.CompassDirection.E,\n",
    "    nethack.CompassDirection.S,\n",
    "    nethack.CompassDirection.W,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "2ca67787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"MiniHack-MazeWalk-9x9-v0\", observation_keys=[\"glyphs\",\"pixel\"],\n",
    "             # ) #reward_manager= reward_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "342d8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-MazeWalk-Mapped-9x9-v0\", observation_keys=[\"glyphs\",\"pixel\"], actions=NAVIGATE_ACTIONS,\n",
    "              max_episode_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "bfcd3d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "aeab06f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=1264x336 at 0x172BA5460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_screen(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "71c04aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(state):\n",
    "    # Flatten the 2D array \n",
    "    glyphs = state[\"glyphs\"].flatten()\n",
    "    #glyphs = state[\"glyphs\"]\n",
    "    # Normalize\n",
    "    glyphs = glyphs/glyphs.max()\n",
    "    \n",
    "    glyphs = glyphs.reshape((1,-1,len(glyphs)))\n",
    "    return glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "e182953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(state):\n",
    "    # Flatten the 2D array \n",
    "    #glyphs = state[\"glyphs\"].flatten()\n",
    "    glyphs = state[\"glyphs\"]\n",
    "    # Normalize\n",
    "    glyphs = glyphs/glyphs.max()\n",
    "    glyphs = glyphs.reshape((1,1,21,79))\n",
    "    return glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e6efea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "b2106b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdim = format_state(env.reset()).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "0309c564",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: -1.16 Average Reward: -1.16 Steps 1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_93569/1839236598.py:53: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  value_loss = F.smooth_l1_loss(value, reward)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Reward: 0.97 Average Reward: -0.09499999999999997 Steps 99\n",
      "Episode: 2 Reward: 0.99 Average Reward: 0.26666666666666666 Steps 79\n",
      "Episode: 3 Reward: 0.92 Average Reward: 0.43000000000000005 Steps 70\n",
      "Episode: 4 Reward: 0.79 Average Reward: 0.502 Steps 59\n",
      "Episode: 5 Reward: 0.81 Average Reward: 0.5533333333333333 Steps 65\n",
      "Episode: 6 Reward: 0.74 Average Reward: 0.5800000000000001 Steps 62\n",
      "Episode: 7 Reward: 0.78 Average Reward: 0.605 Steps 52\n",
      "Episode: 8 Reward: 0.5399999999999999 Average Reward: 0.5977777777777777 Steps 78\n",
      "Episode: 9 Reward: 0.6699999999999999 Average Reward: 0.605 Steps 63\n",
      "Episode: 10 Reward: 0.85 Average Reward: 0.6272727272727272 Steps 49\n",
      "Episode: 11 Reward: 0.96 Average Reward: 0.6549999999999999 Steps 36\n",
      "Episode: 12 Reward: 0.99 Average Reward: 0.6807692307692308 Steps 31\n",
      "Episode: 13 Reward: 0.93 Average Reward: 0.6985714285714285 Steps 39\n",
      "Episode: 14 Reward: 0.98 Average Reward: 0.7173333333333333 Steps 30\n",
      "Episode: 15 Reward: 0.98 Average Reward: 0.73375 Steps 30\n",
      "Episode: 16 Reward: 0.96 Average Reward: 0.7470588235294118 Steps 34\n",
      "Episode: 17 Reward: 0.97 Average Reward: 0.7594444444444445 Steps 33\n",
      "Episode: 18 Reward: 0.92 Average Reward: 0.7678947368421053 Steps 38\n",
      "Episode: 19 Reward: 0.88 Average Reward: 0.7735000000000001 Steps 46\n",
      "Episode: 20 Reward: 1.0 Average Reward: 0.7842857142857143 Steps 28\n",
      "Episode: 21 Reward: 0.92 Average Reward: 0.7904545454545455 Steps 36\n",
      "Episode: 22 Reward: 0.98 Average Reward: 0.7986956521739131 Steps 34\n",
      "Episode: 23 Reward: 0.99 Average Reward: 0.8066666666666666 Steps 33\n",
      "Episode: 24 Reward: 0.89 Average Reward: 0.81 Steps 43\n",
      "Episode: 25 Reward: 0.96 Average Reward: 0.8157692307692308 Steps 32\n",
      "Episode: 26 Reward: 1.0 Average Reward: 0.8225925925925927 Steps 32\n",
      "Episode: 27 Reward: 0.86 Average Reward: 0.8239285714285715 Steps 44\n",
      "Episode: 28 Reward: 0.91 Average Reward: 0.8268965517241379 Steps 43\n",
      "Episode: 29 Reward: 0.98 Average Reward: 0.8320000000000001 Steps 32\n",
      "Episode: 30 Reward: 0.99 Average Reward: 0.8370967741935483 Steps 29\n",
      "Episode: 31 Reward: 1.0 Average Reward: 0.8421875 Steps 30\n",
      "Episode: 32 Reward: 0.9 Average Reward: 0.8439393939393939 Steps 38\n",
      "Episode: 33 Reward: 0.89 Average Reward: 0.8452941176470588 Steps 39\n",
      "Episode: 34 Reward: 0.98 Average Reward: 0.8491428571428571 Steps 30\n",
      "Episode: 35 Reward: 0.93 Average Reward: 0.8513888888888889 Steps 37\n",
      "Episode: 36 Reward: 0.96 Average Reward: 0.8543243243243243 Steps 34\n",
      "Episode: 37 Reward: 0.98 Average Reward: 0.8576315789473683 Steps 30\n",
      "Episode: 38 Reward: 0.93 Average Reward: 0.8594871794871793 Steps 37\n",
      "Episode: 39 Reward: 0.9 Average Reward: 0.8605 Steps 44\n",
      "Episode: 40 Reward: 0.95 Average Reward: 0.8626829268292684 Steps 33\n",
      "Episode: 41 Reward: 0.74 Average Reward: 0.859761904761905 Steps 54\n",
      "Episode: 42 Reward: 0.96 Average Reward: 0.8620930232558142 Steps 32\n",
      "Episode: 43 Reward: 0.97 Average Reward: 0.8645454545454547 Steps 33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_93569/2991698874.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m policy, scores = actor_critic(env=env, model= model, seed=42, learning_rate=0.02, betas=(0.9, 0.999),\n\u001b[0m\u001b[1;32m      3\u001b[0m                            number_episodes=100, max_episode_length=10000, gamma=1 ,verbose=True, max_reward=1)\n",
      "\u001b[0;32m/var/folders/wm/vfztc0rx29d3qrfj3k_2nkgm0000gn/T/ipykernel_93569/1839236598.py\u001b[0m in \u001b[0;36mactor_critic\u001b[0;34m(env, model, seed, learning_rate, betas, number_episodes, max_episode_length, gamma, max_reward, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/minihack/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# Within this call, _is_episode_end is called and then _reward_fn,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# both using self.reward_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_episode_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/nle/env/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_ascended\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhow_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnethack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mASCENDED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_collect_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/minihack/base.py\u001b[0m in \u001b[0;36m_get_observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"pixel\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minihack_obs_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             obs_dict[\"pixel\"] = self._glyph_mapper.to_rgb(\n\u001b[0m\u001b[1;32m    397\u001b[0m                 \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"glyphs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/minihack/tiles/glyph_mapper.py\u001b[0m in \u001b[0;36mto_rgb\u001b[0;34m(self, glyphs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglyphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_glyph_to_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglyphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/minihack/lib/python3.9/site-packages/minihack/tiles/glyph_mapper.py\u001b[0m in \u001b[0;36m_glyph_to_rgb\u001b[0;34m(self, glyphs)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ActorCritic(h_size=128, input_dim = 128, a_size=env.action_space.n)\n",
    "policy, scores = actor_critic(env=env, model= model, seed=42, learning_rate=0.02, betas=(0.9, 0.999),\n",
    "                           number_episodes=100, max_episode_length=10000, gamma=1 ,verbose=True, max_reward=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f9600bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1698917c0>]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8e502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:minihack] *",
   "language": "python",
   "name": "conda-env-minihack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
